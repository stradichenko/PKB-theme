{{- /* Hugo robots.txt template - replaces static/robots.txt */ -}}
{{- $baseURL := site.BaseURL -}}
{{- $seoData := site.Data.seo -}}

# PKB Theme Robots.txt (Generated by Hugo)
# This file tells search engine crawlers which pages to crawl and which to avoid

# Allow all crawlers access to all content by default
User-agent: *
Allow: /

# Disallow crawling of admin, development, and system directories
Disallow: /admin/
Disallow: /.git/
Disallow: /node_modules/
Disallow: /src/
Disallow: /layouts/
Disallow: /data/
Disallow: /config/

# Disallow crawling of draft content and private sections
Disallow: /drafts/
Disallow: /private/
Disallow: /_drafts/

# Disallow crawling of search results and filtered views
Disallow: /search?
Disallow: /*?query=
Disallow: /*?filter=
Disallow: /*?page=

# Disallow crawling of duplicate content with parameters
Disallow: /*?utm_*
Disallow: /*?ref=
Disallow: /*?source=

# Allow crawling of important files
Allow: /robots.txt
Allow: /sitemap.xml
Allow: /sitemap*.xml
Allow: /.well-known/

{{- /* Optional crawl delay from site data */ -}}
{{- with $seoData.robots.crawl_delay }}
# Crawl delay (be respectful to servers)
Crawl-delay: {{ . }}
{{- end }}

# Sitemap location (Hugo generates this automatically)
Sitemap: {{ $baseURL }}sitemap.xml

# Specific rules for different crawlers
# Google Bot - no special restrictions
User-agent: Googlebot
Allow: /

# Bing Bot - no special restrictions  
User-agent: Bingbot
Allow: /

{{- /* Block aggressive crawlers if configured */ -}}
{{- with $seoData.robots.blocked_crawlers }}
{{- range . }}
# Block {{ . }}
User-agent: {{ . }}
Disallow: /
{{- end }}
{{- end }}

{{- /* Custom disallow rules from site data */ -}}
{{- with $seoData.robots.custom_disallow }}
{{- range . }}
# Custom disallow rule
{{ . }}
{{- end }}
{{- end }}
